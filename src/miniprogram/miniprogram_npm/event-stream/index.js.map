{"version":3,"sources":["index.js"],"names":[],"mappings":";;;;;;;AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA","file":"index.js","sourcesContent":["//filter will reemit the data if cb(err,pass) pass is truthy\n\n// reduce is more tricky\n// maybe we want to group the reductions or emit progress updates occasionally\n// the most basic reduce just emits one 'data' event after it has recieved 'end'\n\nvar Stream = require('stream').Stream\n  , es = exports\n  , through = require('through')\n  , from = require('from')\n  , duplex = require('duplexer')\n  , map = require('map-stream')\n  , pause = require('pause-stream')\n  , split = require('split')\n  , pipeline = require('stream-combiner')\n  , immediately = global.setImmediate || process.nextTick;\n\nes.Stream = Stream //re-export Stream from core\nes.through = through\nes.from = from\nes.duplex = duplex\nes.map = map\nes.pause = pause\nes.split = split\nes.pipeline = es.connect = es.pipe = pipeline\n// merge / concat\n//\n// combine multiple streams into a single stream.\n// will emit end only once\n\nes.concat = //actually this should be called concat\nes.merge = function (/*streams...*/) {\n  var toMerge = [].slice.call(arguments)\n  if (toMerge.length === 1 && (toMerge[0] instanceof Array)) {\n    toMerge = toMerge[0] //handle array as arguments object\n  }\n  var stream = new Stream()\n  stream.setMaxListeners(0) // allow adding more than 11 streams\n  var endCount = 0\n  stream.writable = stream.readable = true\n\n  if (toMerge.length) {\n    toMerge.forEach(function (e) {\n      e.pipe(stream, {end: false})\n      var ended = false\n      e.on('end', function () {\n        if(ended) return\n        ended = true\n        endCount ++\n        if(endCount == toMerge.length)\n          stream.emit('end')\n      })\n    })\n  } else {\n    process.nextTick(function () {\n      stream.emit('end')\n    })\n  }\n  \n  stream.write = function (data) {\n    this.emit('data', data)\n  }\n  stream.destroy = function () {\n    toMerge.forEach(function (e) {\n      if(e.destroy) e.destroy()\n    })\n  }\n  return stream\n}\n\n\n// writable stream, collects all events into an array\n// and calls back when 'end' occurs\n// mainly I'm using this to test the other functions\n\nes.collect =\nes.writeArray = function (done) {\n  if ('function' !== typeof done)\n    throw new Error('function writeArray (done): done must be function')\n\n  var a = new Stream ()\n    , array = [], isDone = false\n  a.write = function (l) {\n    array.push(l)\n  }\n  a.end = function () {\n    isDone = true\n    done(null, array)\n  }\n  a.writable = true\n  a.readable = false\n  a.destroy = function () {\n    a.writable = a.readable = false\n    if(isDone) return\n    done(new Error('destroyed before end'), array)\n  }\n  return a\n}\n\n//return a Stream that reads the properties of an object\n//respecting pause() and resume()\n\nes.readArray = function (array) {\n  var stream = new Stream()\n    , i = 0\n    , paused = false\n    , ended = false\n\n  stream.readable = true\n  stream.writable = false\n\n  if(!Array.isArray(array))\n    throw new Error('event-stream.read expects an array')\n\n  stream.resume = function () {\n    if(ended) return\n    paused = false\n    var l = array.length\n    while(i < l && !paused && !ended) {\n      stream.emit('data', array[i++])\n    }\n    if(i == l && !ended)\n      ended = true, stream.readable = false, stream.emit('end')\n  }\n  process.nextTick(stream.resume)\n  stream.pause = function () {\n     paused = true\n  }\n  stream.destroy = function () {\n    ended = true\n    stream.emit('close')\n  }\n  return stream\n}\n\n//\n// readable (asyncFunction)\n// return a stream that calls an async function while the stream is not paused.\n//\n// the function must take: (count, callback) {...\n//\n\nes.readable =\nfunction (func, continueOnError) {\n  var stream = new Stream()\n    , i = 0\n    , paused = false\n    , ended = false\n    , reading = false\n\n  stream.readable = true\n  stream.writable = false\n\n  if('function' !== typeof func)\n    throw new Error('event-stream.readable expects async function')\n\n  stream.on('end', function () { ended = true })\n\n  function get (err, data) {\n\n    if(err) {\n      stream.emit('error', err)\n      if(!continueOnError) stream.emit('end')\n    } else if (arguments.length > 1)\n      stream.emit('data', data)\n\n    immediately(function () {\n      if(ended || paused || reading) return\n      try {\n        reading = true\n        func.call(stream, i++, function () {\n          reading = false\n          get.apply(null, arguments)\n        })\n      } catch (err) {\n        stream.emit('error', err)\n      }\n    })\n  }\n  stream.resume = function () {\n    paused = false\n    get()\n  }\n  process.nextTick(get)\n  stream.pause = function () {\n     paused = true\n  }\n  stream.destroy = function () {\n    stream.emit('end')\n    stream.emit('close')\n    ended = true\n  }\n  return stream\n}\n\n\n//\n// map sync\n//\n\nes.mapSync = function (sync) {\n  return es.through(function write(data) {\n    var mappedData\n    try {\n      mappedData = sync(data)\n    } catch (err) {\n      return this.emit('error', err)\n    }\n    if (mappedData !== undefined)\n      this.emit('data', mappedData)\n  })\n}\n\n//\n// filterSync\n//\n\nes.filterSync = function (test) {\n  return es.through(function(data){\n    var s = this\n    if (test(data)) {\n      s.queue(data)\n    }\n  });\n}\n\n//\n// flatmapSync\n//\n\nes.flatmapSync = function (mapper) {\n  return es.through(function(data) {\n    var s = this\n    data.forEach(function(e) {\n      s.queue(mapper(e))\n    })\n  })\n}\n\n//\n// log just print out what is coming through the stream, for debugging\n//\n\nes.log = function (name) {\n  return es.through(function (data) {\n    var args = [].slice.call(arguments)\n    if(name) console.error(name, data)\n    else     console.error(data)\n    this.emit('data', data)\n  })\n}\n\n\n//\n// child -- pipe through a child process\n//\n\nes.child = function (child) {\n\n  return es.duplex(child.stdin, child.stdout)\n\n}\n\n//\n// parse\n//\n// must be used after es.split() to ensure that each chunk represents a line\n// source.pipe(es.split()).pipe(es.parse())\n\nes.parse = function (options) {\n  var emitError = !!(options ? options.error : false)\n  return es.through(function (data) {\n    var obj\n    try {\n      if(data) //ignore empty lines\n        obj = JSON.parse(data.toString())\n    } catch (err) {\n      if (emitError)\n        return this.emit('error', err)\n      return console.error(err, 'attempting to parse:', data)\n    }\n    //ignore lines that where only whitespace.\n    if(obj !== undefined)\n      this.emit('data', obj)\n  })\n}\n//\n// stringify\n//\n\nes.stringify = function () {\n  var Buffer = require('buffer').Buffer\n  return es.mapSync(function (e){\n    return JSON.stringify(Buffer.isBuffer(e) ? e.toString() : e) + '\\n'\n  })\n}\n\n//\n// replace a string within a stream.\n//\n// warn: just concatenates the string and then does str.split().join().\n// probably not optimal.\n// for smallish responses, who cares?\n// I need this for shadow-npm so it's only relatively small json files.\n\nes.replace = function (from, to) {\n  return es.pipeline(es.split(from), es.join(to))\n}\n\n//\n// join chunks with a joiner. just like Array#join\n// also accepts a callback that is passed the chunks appended together\n// this is still supported for legacy reasons.\n//\n\nes.join = function (str) {\n\n  //legacy api\n  if('function' === typeof str)\n    return es.wait(str)\n\n  var first = true\n  return es.through(function (data) {\n    if(!first)\n      this.emit('data', str)\n    first = false\n    this.emit('data', data)\n    return true\n  })\n}\n\n\n//\n// wait. callback when 'end' is emitted, with all chunks appended as string.\n//\n\nes.wait = function (callback) {\n  var arr = []\n  return es.through(function (data) { arr.push(data) },\n    function () {\n      var body = Buffer.isBuffer(arr[0]) ? Buffer.concat(arr)\n        : arr.join('')\n      this.emit('data', body)\n      this.emit('end')\n      if(callback) callback(null, body)\n    })\n}\n\nes.pipeable = function () {\n  throw new Error('[EVENT-STREAM] es.pipeable is deprecated')\n}\n"]}